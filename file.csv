TITLE,AUTHORS NAMES,ABSTRACT,LINK
Exponential Bellman Equation and Improved Regret Bounds for Risk-Sensitive Reinforcement Learning,"Yingjie Fei, Zhuoran Yang, Yudong Chen, Zhaoran Wang","We study risk-sensitive reinforcement learning (RL) based on the entropic risk measure. Although existing works have established non-asymptotic regret guarantees for this problem, they leave open an exponential gap between the upper and lower bounds. We identify the deficiencies in existing algorithms and their analysis that result in such a gap. To remedy these deficiencies, we investigate a simple transformation of the risk-sensitive Bellman equations, which we call the exponential Bellman equation. The exponential Bellman equation inspires us to develop a novel analysis of Bellman backup procedures in risk-sensitive RL algorithms, and further motivates the design of a novel exploration mechanism. We show that these analytic and algorithmic innovations together lead to improved regret upper bounds over existing ones.",https://www.semanticscholar.org/paper/Exponential-Bellman-Equation-and-Improved-Regret-Fei-Yang/3cb13b61804d5ff0d96c7cca127087171c880beb
Variants of Bellman equation on reinforcement learning problems,Zhen-Yi Zhao,"Reinforcement learning is a popular tool nowadays applied in a wide range of fields. Markov Decision Process is the basis of reinforcement learning and the Bellman equation derived from the value function of the Markov decision process has become a fundamental tool for finding optimal policies. However, the application of the basic Bellman formula has significant limitations and is computationally complex, resulting in high time costs and low efficiency. Therefore, there are different variants of the Bellman formula and various solutions to different problems. These solutions include MonteCarlo method, Temporal-Difference learning, Sarsa algorithm, Q-learning algorithm, Deep Q-Networks, Hamilton-Jacobi-Bellman equation and others. This paper reviews these algorithms and compares the complexity of each algorithm.",https://www.semanticscholar.org/paper/Variants-of-Bellman-equation-on-reinforcement-Zhao/ba56b43c34d6161effed557880b10b5db074afca
Exponential TD Learning: A Risk-Sensitive Actor-Critic Reinforcement Learning Algorithm,"Erfaun Noorani, Christos N. Mavridis, J. Baras","Incorporating risk in the decision-making process has been shown to lead to significant performance improvement in optimal control and reinforcement learning algorithms. We construct a temporal-difference risk-sensitive reinforcement learning algorithm using the exponential criteria commonly used in risk-sensitive control. The proposed method resembles an actor-critic architecture with the ‘actor’ implementing a policy gradient algorithm based on the exponential of the reward-to-go, which is estimated by the ‘critic’. The novelty of the update rule of the ‘critic’ lies in the use of a modified objective function that corresponds to the underlying multiplicative Bellman’s equation. Our results suggest that the use of the exponential criteria accelerates the learning process and reduces its variance, i.e., risk-sensitiveness can be utilized by actor-critic methods and can lead to improved performance.",https://www.semanticscholar.org/paper/Exponential-TD-Learning%3A-A-Risk-Sensitive-Learning-Noorani-Mavridis/424133b1bd28c019f44b80aab0be4530f54577b9
Distributional Hamilton-Jacobi-Bellman Equations for Continuous-Time Reinforcement Learning,"Harley Wiltzer, D. Meger, Marc G. Bellemare","Continuous-time reinforcement learning offers an appealing formalism for describing control problems in which the passage of time is not naturally divided into discrete increments. Here we consider the problem of predicting the distribution of returns obtained by an agent interacting in a continuous-time, stochastic environment. Accurate return predictions have proven useful for determining optimal policies for risk-sensitive control, learning state representations, multiagent coordination, and more. We begin by establishing the distributional analogue of the Hamilton-Jacobi-Bellman (HJB) equation for It\^o diffusions and the broader class of Feller-Dynkin processes. We then specialize this equation to the setting in which the return distribution is approximated by $N$ uniformly-weighted particles, a common design choice in distributional algorithms. Our derivation highlights additional terms due to statistical diffusivity which arise from the proper handling of distributions in the continuous-time setting. Based on this, we propose a tractable algorithm for approximately solving the distributional HJB based on a JKO scheme, which can be implemented in an online control algorithm. We demonstrate the effectiveness of such an algorithm in a synthetic control problem.",https://www.semanticscholar.org/paper/Distributional-Hamilton-Jacobi-Bellman-Equations-Wiltzer-Meger/fc31f432580ba762a31e60ab3a21f0f155fac5b4
Risk-Sensitive Reinforcement Learning with Exponential Criteria,"Erfaun Noorani, Christos N. Mavridis, J. Baras","While reinforcement learning has shown experimental success in a number of applications, it is known to be sensitive to noise and perturbations in the parameters of the system, leading to high variance in the total reward amongst different episodes in slightly different environments. To introduce robustness, as well as sample efficiency, risk-sensitive reinforcement learning methods are being thoroughly studied. In this work, we provide a definition of robust reinforcement learning policies and formulate a risk-sensitive reinforcement learning problem to approximate them, by solving an optimization problem with respect to a modified objective based on exponential criteria. In particular, we study a model-free risk-sensitive variation of the widely-used Monte Carlo Policy Gradient algorithm and introduce a novel risk-sensitive online Actor-Critic algorithm based on solving a multiplicative Bellman equation using stochastic approximation updates. Analytical results suggest that the use of exponential criteria generalizes commonly used ad-hoc regularization approaches, improves sample efficiency, and introduces robustness with respect to perturbations in the model parameters and the environment. The implementation, performance, and robustness properties of the proposed methods are evaluated in simulated experiments.",https://www.semanticscholar.org/paper/Risk-Sensitive-Reinforcement-Learning-with-Criteria-Noorani-Mavridis/f114e0d03a5fda080b1a04e31f8873464a2ed872
Mean-Semivariance Policy Optimization via Risk-Averse Reinforcement Learning,"Xiaoteng Ma, Shuai Ma, Li Xia, Qianchuan Zhao","Keeping risk under control is often more crucial than maximizing expected reward in real-world decision-making situations, such as finance, robotics, autonomous driving, etc. The most natural choice of risk measures is variance, while it penalizes the upside volatility as much as the downside part. Instead, the (downside) semivariance, which captures the negative deviation of a random variable under its mean, is more suitable for risk-averse proposes. This paper aims at optimizing the mean-semivariance (MSV) criterion in reinforcement learning w.r.t. steady rewards. Since semivariance is time-inconsistent and does not satisfy the standard Bellman equation, the traditional dynamic programming methods are inapplicable to MSV problems directly. To tackle this challenge, we resort to the Perturbation Analysis (PA) theory and establish the performance difference formula for MSV. We reveal that the MSV problem can be solved by iteratively solving a sequence of RL problems with a policy-dependent reward function. Further, we propose two on-policy algorithms based on the policy gradient theory and the trust region method. Finally, we conduct diverse experiments from simple bandit problems to continuous control tasks in MuJoCo, which demonstrate the effectiveness of our proposed methods.",https://www.semanticscholar.org/paper/Mean-Semivariance-Policy-Optimization-via-Learning-Ma-Ma/c3a2e7d52d975b50c9e98e6cca4a7449b3068c2f
Bridging Physics-Informed Neural Networks with Reinforcement Learning: Hamilton-Jacobi-Bellman Proximal Policy Optimization (HJBPPO),"Amartya Mukherjee, Jun Liu",This paper introduces the Hamilton-Jacobi-Bellman Proximal Policy Optimization (HJBPPO) algorithm into reinforcement learning. The Hamilton-Jacobi-Bellman (HJB) equation is used in control theory to evaluate the optimality of the value function. Our work combines the HJB equation with reinforcement learning in continuous state and action spaces to improve the training of the value network. We treat the value network as a Physics-Informed Neural Network (PINN) to solve for the HJB equation by computing its derivatives with respect to its inputs exactly. The Proximal Policy Optimization (PPO)-Clipped algorithm is improvised with this implementation as it uses a value network to compute the objective function for its policy network. The HJBPPO algorithm shows an improved performance compared to PPO on the MuJoCo environments.,https://www.semanticscholar.org/paper/Bridging-Physics-Informed-Neural-Networks-with-Mukherjee-Liu/cd5775dfbf2235ee8018f0d513617a72146c25f5
Reinforcement learning beyond the Bellman equation: Exploring critic objectives using evolution,"A. Leite, Madhavun Candadai, E. Izquierdo",Living organisms learn on multiple time scales: evolutionary as well as individual-lifetime learning. These two learning modes are complementary: the innate phenotypes developed through evolution s...,https://www.semanticscholar.org/paper/Reinforcement-learning-beyond-the-Bellman-equation%3A-Leite-Candadai/2c2a2ae0d3e7c7c10b285f6d693921b10ab476a5
Is Risk-Sensitive Reinforcement Learning Properly Resolved?,"Ruiwen Zhou, Minghuan Liu, Kan Ren, Xufang Luo, Weinan Zhang, Dongsheng Li","Due to the nature of risk management in learning applicable policies, risk-sensitive reinforcement learning (RSRL) has been realized as an important direction. RSRL is usually achieved by learning risk-sensitive objectives characterized by various risk measures, under the framework of distributional reinforcement learning. However, it remains unclear if the distributional Bellman operator properly optimizes the RSRL objective in the sense of risk measures. In this paper, we prove that the existing RSRL methods do not achieve unbiased optimization and can not guarantee optimality or even improvements regarding risk measures over accumulated return distributions. To remedy this issue, we further propose a novel algorithm, namely Trajectory Q-Learning (TQL), for RSRL problems with provable convergence to the optimal policy. Based on our new learning architecture, we are free to introduce a general and practical implementation for different risk measures to learn disparate risk-sensitive policies. In the experiments, we verify the learnability of our algorithm and show how our method effectively achieves better performances toward risk-sensitive objectives.",https://www.semanticscholar.org/paper/Is-Risk-Sensitive-Reinforcement-Learning-Properly-Zhou-Liu/05bde4269ebeef94d3ccf7248d12b1b9e4b8edfa
Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion,"Taehyun Cho, Seung Han, Heesoo Lee, Kyungjae Lee, Jungwoo Lee","Distributional reinforcement learning algorithms have attempted to utilize estimated uncertainty for exploration, such as optimism in the face of uncertainty. However, using the estimated variance for optimistic exploration may cause biased data collection and hinder convergence or performance. In this paper, we present a novel distributional reinforcement learning algorithm that selects actions by randomizing risk criterion to avoid one-sided tendency on risk. We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property. Our theoretical results support that the proposed method does not fall into biased exploration and is guaranteed to converge to an optimal return. Finally, we empirically show that our method outperforms other existing distribution-based algorithms in various environments including Atari 55 games.",https://www.semanticscholar.org/paper/Pitfall-of-Optimism%3A-Distributional-Reinforcement-Cho-Han/034b015f0a8e8f1921e2e4d8c854c939228c4e02
Risk-Aware Distributed Multi-Agent Reinforcement Learning,"A. Maruf, Luyao Niu, Bhaskar Ramasubramanian, Andrew Clark, R. Poovendran","Autonomous cyber and cyber-physical systems need to perform decision-making, learning, and control in unknown environments. Such decision-making can be sensitive to multiple factors, including modeling errors, changes in costs, and impacts of events in the tails of probability distributions. Although multi-agent reinforcement learning (MARL) provides a framework for learning behaviors through repeated interactions with the environment by minimizing an average cost, it will not be adequate to overcome the above challenges. In this paper, we develop a distributed MARL approach to solve decision-making problems in unknown environments by learning risk-aware actions. We use the conditional value-at-risk (CVaR) to characterize the cost function that is being minimized, and define a Bellman operator to characterize the value function associated to a given state-action pair. We prove that this operator satisfies a contraction property, and that it converges to the optimal value function. We then propose a distributed MARL algorithm called the CVaR QD-Learning algorithm, and establish that value functions of individual agents reaches consensus. We identify several challenges that arise in the implementation of the CVaR QD-Learning algorithm, and present solutions to overcome these. We evaluate the CVaR QD-Learning algorithm through simulations, and demonstrate the effect of a risk parameter on value functions at consensus.",https://www.semanticscholar.org/paper/Risk-Aware-Distributed-Multi-Agent-Reinforcement-Maruf-Niu/02b78a9c5df4202fa5b364c3712df86e742bae5c
Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning under Distribution Shifts,"Tobias Enders, James Harrison, Maximilian Schiffer","We study the robustness of deep reinforcement learning algorithms against distribution shifts within contextual multi-stage stochastic combinatorial optimization problems from the operations research domain. In this context, risk-sensitive algorithms promise to learn robust policies. While this field is of general interest to the reinforcement learning community, most studies up-to-date focus on theoretical results rather than real-world performance. With this work, we aim to bridge this gap by formally deriving a novel risk-sensitive deep reinforcement learning algorithm while providing numerical evidence for its efficacy. Specifically, we introduce discrete Soft Actor-Critic for the entropic risk measure by deriving a version of the Bellman equation for the respective Q-values. We establish a corresponding policy improvement result and infer a practical algorithm. We introduce an environment that represents typical contextual multi-stage stochastic combinatorial optimization problems and perform numerical experiments to empirically validate our algorithm's robustness against realistic distribution shifts, without compromising performance on the training distribution. We show that our algorithm is superior to risk-neutral Soft Actor-Critic as well as to two benchmark approaches for robust deep reinforcement learning. Thereby, we provide the first structured analysis on the robustness of reinforcement learning under distribution shifts in the realm of contextual multi-stage stochastic combinatorial optimization problems.",https://www.semanticscholar.org/paper/Risk-Sensitive-Soft-Actor-Critic-for-Robust-Deep-Enders-Harrison/64a77a96c35261321b106387f53c2b1dfd1506da
Machine Learning and Hamilton-Jacobi-Bellman Equation for Optimal Decumulation: a Comparison Study,"Marc-André P. Chen, M. Shirazi, P. Forsyth, Yuying Li","We propose a novel data-driven neural network (NN) optimization framework for solving an optimal stochastic control problem under stochastic constraints. Customized activation functions for the output layers of the NN are applied, which permits training via standard unconstrained optimization. The optimal solution yields a multi-period asset allocation and decumulation strategy for a holder of a defined contribution (DC) pension plan. The objective function of the optimal control problem is based on expected wealth withdrawn (EW) and expected shortfall (ES) that directly targets left-tail risk. The stochastic bound constraints enforce a guaranteed minimum withdrawal each year. We demonstrate that the data-driven approach is capable of learning a near-optimal solution by benchmarking it against the numerical results from a Hamilton-Jacobi-Bellman (HJB) Partial Differential Equation (PDE) computational framework.",https://www.semanticscholar.org/paper/Machine-Learning-and-Hamilton-Jacobi-Bellman-for-a-Chen-Shirazi/5ac4253a12ac0713937dbc75c9bd457cc75e4b6a
Bellman Meets Hawkes: Model-Based Reinforcement Learning via Temporal Point Processes,"C. Qu, Xiaoyu Tan, Siqiao Xue, X. Shi, James Zhang, Hongyuan Mei","We consider a sequential decision making problem where the agent faces the environment characterized by the stochastic discrete events and seeks an optimal intervention policy such that its long-term reward is maximized. This problem exists ubiquitously in social media, finance and health informatics but is rarely investigated by the conventional research in reinforcement learning. To this end, we present a novel framework of the model-based reinforcement learning where the agent's actions and observations are asynchronous stochastic discrete events occurring in continuous-time. We model the dynamics of the environment by Hawkes process with external intervention control term and develop an algorithm to embed such process in the Bellman equation which guides the direction of the value gradient. We demonstrate the superiority of our method in both synthetic simulator and real-data experiments.",https://www.semanticscholar.org/paper/Bellman-Meets-Hawkes%3A-Model-Based-Reinforcement-via-Qu-Tan/7cf779d889dbf155e089289bab1495be2b186b11
Distributional Reinforcement Learning for Risk-Sensitive Policies,"Shiau Hong Lim, Ilyas Malik","We address the problem of learning a risk-sensitive policy based on the CVaR risk measure using distributional reinforcement learning. In particular, we show that the standard action-selection strategy when applying the distributional Bellman optimality operator can result in convergence to neither the dynamic, Markovian CVaR nor the static, non-Markovian CVaR. We propose modiﬁcations to the existing algorithms that include a new distributional Bellman operator and show that the proposed strategy greatly expands the utility of distributional RL in learning and representing CVaR-optimized policies. Our proposed approach is a simple extension of standard distributional RL algorithms and can therefore take advantage of many of the recent advances in deep RL. On both synthetic and real data, we empirically show that our proposed algorithm is able to learn better CVaR-optimized policies",https://www.semanticscholar.org/paper/Distributional-Reinforcement-Learning-for-Policies-Lim-Malik/d1164285427613fa70132d2756347555984e828a
Model-Free Characterizations of the Hamilton-Jacobi-Bellman Equation and Convex Q-Learning in Continuous Time,"F. Lu, J. Mathias, Sean P. Meyn, K. Kalsi","Convex Q-learning is a recent approach to reinforcement learning, motivated by the possibility of a ﬁrmer theory for convergence, and the possibility of making use of greater a priori knowledge regarding policy or value function structure. This paper explores algorithm design in the continuous time domain, with ﬁnite-horizon optimal control objective. The main contributions are (i) Algorithm design is based on a new Q-ODE , which deﬁnes the model-free characterization of the Hamilton-Jacobi-Bellman equation. (ii) The Q-ODE motivates a new formulation of Convex Q-learning that avoids the approximations appearing in prior work. The Bellman error used in the algorithm is deﬁned by ﬁltered measurements, which is beneﬁcial in the presence of measurement noise. (iii) A characterization of boundedness of the constraint region is obtained through a non-trivial extension of recent results from the discrete time setting. (iv) The theory is illustrated in application to resource allocation for distributed energy resources, for which the theory is ideally suited.",https://www.semanticscholar.org/paper/Model-Free-Characterizations-of-the-Equation-and-in-Lu-Mathias/97b25715fa1df6322776778b8d71996e2d2d0535
Finite Sample Analysis of Mean-Volatility Actor-Critic for Risk-Averse Reinforcement Learning,"Khaled Eldowa, L. Bisi, Marcello Restelli","The goal in the standard reinforcement learning problem is to find a policy that optimizes the expected return. However, such an ob-jective is not adequate in a lot of real-life applications, like finance, where controlling the uncertainty of the outcome is imperative. The mean-volatility objective penalizes, through a tunable parameter, policies with high variance of the per-step reward. An interesting property of this objective is that it admits simple linear Bellman equations that resemble, up to a reward transformation, those of the risk-neutral case. However, the required reward transformation is policy-dependent, and requires the (usually unknown) expected return of the used policy. In this work, we propose two general meth-ods for policy evaluation under the mean-volatility objective: the direct method and the factored method. We then extend recent results for finite sample analysis in the risk-neutral actor-critic setting to the mean-volatility case. Our analysis shows that the sample complexity to attain an ϵ -accurate stationary point is the same as that of the risk-neutral version, using either policy evaluation method for training the critic. Finally, we carry out experiments to test the proposed methods in a simple environment that exhibits some trade-off between optimality, in expectation, and uncertainty of outcome.",https://www.semanticscholar.org/paper/Finite-Sample-Analysis-of-Mean-Volatility-for-Eldowa-Bisi/670c8e9c4b7f673b97efd4fe0e0d6379c0522f7b
Risk-Averse Reinforcement Learning via Dynamic Time-Consistent Risk Measures,"Xian Yu, Siqian Shen","Traditional reinforcement learning (RL) aims to maximize the expected total reward, while the risk of uncertain outcomes needs to be controlled to ensure reliable performance in a risk-averse setting. In this paper, we consider the problem of maximizing dynamic risk of a sequence of rewards in infinite-horizon Markov Decision Processes (MDPs). We adapt the Expected Conditional Risk Measures (ECRMs) to the infinite-horizon risk-averse MDP and prove its time consistency. Using a convex combination of expectation and conditional value-at-risk (CVaR) as a special one-step conditional risk measure, we reformulate the risk-averse MDP as a risk-neutral counterpart with augmented action space and manipulation on the immediate rewards. We further prove that the related Bellman operator is a contraction mapping, which guarantees the convergence of any value-based RL algorithms. Accordingly, we develop a risk-averse deep Q-learning framework, and our numerical studies based on two simple MDPs show that the risk-averse setting can reduce the variance and enhance robustness of the results.",https://www.semanticscholar.org/paper/Risk-Averse-Reinforcement-Learning-via-Dynamic-Risk-Yu-Shen/497cb33421ba712d4ec4270037db51962972fae5
Uncertainty-Aware Reinforcement Learning for Risk-Sensitive Player Evaluation in Sports Game,"Guiliang Liu, Yudong Luo, O. Schulte, P. Poupart","A major task of sports analytics is player evaluation. Previous methods commonly measured the impact of players’ actions on desirable outcomes (e.g., goals or winning) without considering the risk induced by stochastic game dynamics. In this paper, we design an uncertainty-aware Reinforcement Learning (RL) framework to learn a risk-sensitive player evaluation metric from stochastic game dynamics. To embed the risk of a player’s movements into the distribution of action-values, we model their 1) aleatoric uncertainty , which represents the intrinsic stochasticity in a sports game, and 2) epistemic uncertainty , which is due to a model’s insufﬁcient knowledge regarding Out-of-Distribution (OoD) samples. We demonstrate how a distributional Bellman operator and a feature-space density model can capture these uncertainties. Based on such uncertainty estimation, we propose a Risk-sensitive Game Impact Metric (RiGIM) that measures players’ performance over a season by conditioning on a speciﬁc conﬁdence level. Empirical evaluation, based on over 9M play-by-play ice hockey and soccer events, shows that RiGIM correlates highly with standard success measures and has a consistent risk sensitivity.",https://www.semanticscholar.org/paper/Uncertainty-Aware-Reinforcement-Learning-for-Player-Liu-Luo/7d4b8e25861f0813d16784a5f679930b364df0a0
Prescribed-Time Formation Control for a Class of Multiagent Systems via Fuzzy Reinforcement Learning,"Yan Zhang, M. Chadli, Z. Xiang","This article concerns optimal prescribed-time formation control for a class of nonlinear multiagent systems (MASs). Optimal control depends on the solution of the Hamilton–Jacobi–Bellman equation, which is hard to be calculated directly due to its inherent nonlinearity. To overcome this difficulty, the reinforcement learning strategy with fuzzy logic systems is proposed, in which identifier, actor, and critic are used to estimate unknown nonlinear dynamics, implement control behavior, and evaluate system performance, respectively. Different from the existing optimal control algorithms, a new performance index function considering formation error cost and control input energy cost is constructed to achieve optimal formation control of MASs within a prescribed time. The presented control strategy can ensure that the formation error converges to the desired accuracy within a prescribed time. Finally, the validity of the presented strategy is verified via a simulation example.",https://www.semanticscholar.org/paper/Prescribed-Time-Formation-Control-for-a-Class-of-Zhang-Chadli/dc1ff8a137bc96f8959e81f2df5e9192e85b5dcb
Risk Bounds and Rademacher Complexity in Batch Reinforcement Learning,"Yaqi Duan, Chi Jin, Zhiyuan Li","This paper considers batch Reinforcement Learning (RL) with general value function approximation. Our study investigates the minimal assumptions to reliably estimate/minimize Bellman error, and characterizes the generalization performance by (local) Rademacher complexities of general function classes, which makes initial steps in bridging the gap between statistical learning theory and batch RL. Concretely, we view the Bellman error as a surrogate loss for the optimality gap, and prove the followings: (1) In double sampling regime, the excess risk of Empirical Risk Minimizer (ERM) is bounded by the Rademacher complexity of the function class. (2) In the single sampling regime, sample-efficient risk minimization is not possible without further assumptions, regardless of algorithms. However, with completeness assumptions, the excess risk of FQI and a minimax style algorithm can be again bounded by the Rademacher complexity of the corresponding function classes. (3) Fast statistical rates can be achieved by using tools of local Rademacher complexity. Our analysis covers a wide range of function classes, including finite classes, linear spaces, kernel spaces, sparse linear features, etc.",https://www.semanticscholar.org/paper/Risk-Bounds-and-Rademacher-Complexity-in-Batch-Duan-Jin/2adfead37e5fba25cdcde27c31dca117e0b1e2f0
Choquet Regularization for Continuous-Time Reinforcement Learning,"Xia Han, Ruodu Wang, Xun Yu Zhou",". We propose Choquet regularizers to measure and manage the level of exploration for 4 reinforcement learning (RL), and reformulate the continuous-time entropy-regularized RL problem 5 of [47] in which we replace the differential entropy used for regularization with a Choquet regular-6 izer. We derive the Hamilton–Jacobi–Bellman equation of the problem, and solve it explicitly in 7 the linear–quadratic (LQ) case via maximizing statically a mean–variance constrained Choquet reg-8 ularizer. Under the LQ setting, we derive explicit optimal distributions for several specific Choquet 9 regularizers, and conversely identify the Choquet regularizers that generate a number of broadly used 10 exploratory samplers such as ε -greedy,",https://www.semanticscholar.org/paper/Choquet-Regularization-for-Continuous-Time-Learning-Han-Wang/627297f94864d00b796813cfebf6819af145465f
Physical Deep Reinforcement Learning: Safety and Unknown Unknowns,"H. Cao, Y. Mao, L. Sha, M. Caccamo","In this paper, we propose the Phy-DRL: a physics-model-regulated deep reinforcement learning framework for safety-critical autonomous systems. The Phy-DRL is unique in three innovations: i) proactive unknown-unknowns training, ii) conjunctive residual control (i.e., integration of data-driven control and physics-model-based control) and safety- \&stability-sensitive reward, and iii) physics-model-based neural network editing, including link editing and activation editing. Thanks to the concurrent designs, the Phy-DRL is able to 1) tolerate unknown-unknowns disturbances, 2) guarantee mathematically provable safety and stability, and 3) strictly comply with physical knowledge pertaining to Bellman equation and reward. The effectiveness of the Phy-DRL is finally validated by an inverted pendulum and a quadruped robot. The experimental results demonstrate that compared with purely data-driven DRL, Phy-DRL features remarkably fewer learning parameters, accelerated training and enlarged reward, while offering enhanced model robustness and safety assurance.",https://www.semanticscholar.org/paper/Physical-Deep-Reinforcement-Learning%3A-Safety-and-Cao-Mao/2726451ddd17c0e858572943b7937898d22e31cd
On solutions of the distributional Bellman equation,"Julian Gerstenberg, Ralph Neininger, Denis Spiegel","In distributional reinforcement learning (RL), not only expected returns but the complete return distributions of a policy are taken into account. The return distribution for a fixed policy is given as the solution of an associated distributional Bellman equation. In this note, we consider general distributional Bellman equations and study the existence and uniqueness of their solutions, as well as tail properties of return distributions. We give necessary and sufficient conditions for the existence and uniqueness of return distributions and identify cases of regular variation. We link distributional Bellman equations to multivariate affine distributional equations. We show that any solution of a distributional Bellman equation can be obtained as the vector of marginal laws of a solution to a multivariate affine distributional equation. This makes the general theory of such equations applicable to the distributional reinforcement learning setting.",https://www.semanticscholar.org/paper/On-solutions-of-the-distributional-Bellman-equation-Gerstenberg-Neininger/a98a59d4e9c54f9aebf03a661ec5ccdae7b9bb64
Robust dynamic event‐triggered control of saturated nonlinear systems using reinforcement learning,"Wenqian Zheng, Xiong Yang","We develop a robust dynamic event‐triggered control (ETC) law for asymmetrically input‐saturated nonlinear systems having matched disturbances. Initially, by constructing a novel cost function for the associated nominal system, we transform the constrained robust stabilization problem into an unconstrained optimal control problem. Then, we propose a dynamic event‐triggering rule for improving the performance in reducing the computational burden. Based on such a dynamic triggering rule, we present the event‐triggered Hamilton‐Jacobi‐Bellman equation (ET‐HJBE). To solve the ET‐HJBE, we employ a critic approximator with its parameters being updated in the reinforcement learning framework. After that, we apply Lyapunov's direct method to prove uniform ultimate boundedness of the closed‐loop system and the critic approximator's parameter estimation error. Finally, we provide two nonlinear plants to validate the present dynamic ETC law.",https://www.semanticscholar.org/paper/Robust-dynamic-event%E2%80%90triggered-control-of-saturated-Zheng-Yang/7ba10b8539399b140f6a9ba0063fd4e2f09496e8
Mars powered descent phase guidance law based on reinforcement learning for collision avoidance,"Yao Zhang, Tianyi Zeng, Yanning Guo, G. Ma"," This paper proposes a reinforcement  learning ‐based guidance law for Mars powered descent phase, which is an effective online calculation method that handles the nonlinearity caused by the mass variation and avoids collisions. The reinforcement  learning method is designed to solve the constrained nonlinear optimization problem by using a critic neural network. Specifically, to cope with the position constraint (i.e., glide‐slope constraint) and the thrust force limit constraint, a modified cost function is proposed, and the associated Hamilton‐Jacobi‐ Bellman  equation is solved online without using an actor neural network, which significantly reduces the computational burden. The convergence of the critic neural network is proven. Simulation results show the effectiveness of the proposed method. ",https://www.semanticscholar.org/paper/Mars-powered-descent-phase-guidance-law-based-on-Zhang-Zeng/10a7c000407b1e8d072030e491c9e1a48caac025
Online Adaptive Integral Reinforcement Learning for Nonlinear Multi-Input System,"Yongfeng Lv, Huijuan Chang, Jun Zhao","In this brief article, a novel adaptive integral reinforcement learning (AIRL) scheme is proposed for the continuous-time (CT) system. Moreover, it is used to learn the optimal controls of the partially unknown multi-input nonlinear system. Firstly, the Nash equilibrium of multi-input is defined. Two neural networks (NN) are used to approximate the cost functions with the integral reinforcement signal, which can avoid directly solving the Hamilton–Jacobi–Bellman (HJB) equation such that dynamic information and derivatives of NN activations are not needed. Then, a novel learning algorithm is used to update the unknown NN weights. The studied weights are used to obtain the optimum multi-policies. The learned weight convergence is proved. Finally, two examples are presented to verify the system performance with the proposed AIRL scheme.",https://www.semanticscholar.org/paper/Online-Adaptive-Integral-Reinforcement-Learning-for-Lv-Chang/789d6669ae9dd4c5a263d6cc94b93ff4140913ca
LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning,"Outongyi Lv, Bingxin Zhou, Yu Wang","Modern reinforcement learning (RL) can be categorized into online and offline variants. As a pivotal aspect of both online and offline RL, current research on the Bellman equation revolves primarily around optimization techniques and performance enhancement rather than exploring the inherent structural properties of the Bellman error, such as its distribution characteristics. This study investigates the distribution of the Bellman approximation error in both online and offline settings through iterative exploration of the Bellman equation. We observed that both in online RL and offline RL, the Bellman error conforms to a Logistic distribution. Building upon this discovery, this study employed the Logistics maximum likelihood function (LLoss) as an alternative to the commonly used MSE Loss, assuming that Bellman errors adhere to a normal distribution. We validated our hypotheses through extensive numerical experiments across diverse online and offline environments. In particular, we applied corrections to the loss function across various baseline algorithms and consistently observed that the loss function with Logistic corrections outperformed the MSE counterpart significantly. Additionally, we conducted Kolmogorov-Smirnov tests to confirm the reliability of the Logistic distribution. This study's theoretical and empirical insights provide valuable groundwork for future investigations and enhancements centered on the distribution of Bellman errors.",https://www.semanticscholar.org/paper/LLQL%3A-Logistic-Likelihood-Q-Learning-for-Learning-Lv-Zhou/3e72a33046892c13fc8deaa3628e01dc518348e5
Input‐constrained optimal output synchronization of heterogeneous multiagent systems via observer‐based model‐free reinforcement learning,"Tengfei Zhang, Yingmin Jia","This paper presents a new formulation of input‐constrained optimal output synchronization problem and proposes an observer‐based distributed optimal control protocol for discrete‐time heterogeneous multiagent systems with input constraints via model‐free reinforcement learning. First, distributed adaptive observers are designed for all agents to estimate the leader's trajectory without requiring its dynamics knowledge. Subsequently, the optimal control input associated with the optimal value function is derived based on the solution to the tracking Hamilton‐Jacobi‐Bellman equation, which is always difficult to solve analytically. To this end, motivated by reinforcement learning technique, a model‐free Q‐learning policy iteration algorithm is proposed, and the actor‐critic neural network structure is implemented to iteratively find the optimal tracking control input without knowing system dynamics. Moreover, inputs of all agents are constrained in the permitted bounds by inserting a nonquadratic function into the performance function, where input constraints are encoded into the optimization problem. Finally, a numerical simulation example is provided to illustrate the effectiveness of the proposed theoretical results.",https://www.semanticscholar.org/paper/Input%E2%80%90constrained-optimal-output-synchronization-of-Zhang-Jia/1d79e52fe8cb41f27abcf62bee82f07058237241
Research on international logistics supply chain management strategy based on deep reinforcement learning,"Yuzhen Wang, Jian Wang","Abstract The use of deep reinforcement learning algorithms for strategy formulation in supply chain management enables the nodes in the supply chain to better improve their management strategies. In this paper, a supply chain model is constructed as a starting point, and deep reinforcement learning algorithms are introduced on this basis. Firstly, the decision problem of uncertainty is handled by the reinforcement learning method of functions, and the DQN algorithm (deep neural network algorithm) is divided into two parts for iterative rules. Then the target network is established to make the iterative process more stable, to improve the convergence of the algorithm, evaluate the loss function in the training process of the network, and to determine its influence factor. Then the neural network is used to improve the iteration rule, improve the output layer, select the final action, and define the model expectation reward. Finally, the Bellman equation is fitted to the function by a deep neural network to calculate the final result. The experimental results show that by analyzing and constructing the cost of international logistics under supply chain management, the capacity utilization rate of ocean freight link is 57% The unloading link is 74% and the total capacity utilization rate is calculated as 76%. It shows that using deep reinforcement learning algorithms under international logistics supply chain management is feasible and necessary for improving the management strategy research of supply chains.",https://www.semanticscholar.org/paper/Research-on-international-logistics-supply-chain-on-Wang-Wang/d1f4a1cdc1e1f7c6211c73a0c2d1249d6ce4f0dc
A Reinforcement Learning-Based Control Approach for Tracking Problem of a Class of Nonlinear Systems: Applied to a Single-Link Manipulator,"Farshad Rahimi, Sepideh Ziaei, R. M. Esfanjani"," This paper introduces a reinforcement  learning -based tracking control approach for a class of nonlinear systems using neural networks in the presence of adversarial attacks. This approach incorporates a simultaneous tracking and optimization process. It is necessary to be able to solve the Hamilton-Jacobi- Bellman  equation (HJB) in order to obtain optimal control input, but this is difficult due to the strong nonlinearity terms in the equation . In order to find the solution to the HJB equation , we used a reinforcement  learning approach. In this online adaptive learning approach, three neural networks are simultaneously adapted: the critic neural network, the actor neural network, and the adversary neural network. Ultimately, simulation results are presented to demonstrate the effectiveness of the introduced method on a manipulator. ",https://www.semanticscholar.org/paper/A-Reinforcement-Learning-Based-Control-Approach-for-Rahimi-Ziaei/b774f9267607c56fa801f2797ed6957b5839fcda
Empowering Trustworthy Client Selection in Edge Federated Learning Leveraging Reinforcement Learning,"Asadullah Tariq, Abderrahmane Lakas, F. Sallabi, Tariq Qayyum, M. Serhani, Ezedin Baraka","Federated learning (FL) is a promising approach for training AI models across multiple clients in Edge Computing (EC), without sharing raw local data. By enabling local training and aggregating updates into a global model, FL maintains privacy while facilitating collaborative learning. Nevertheless, FL encounters several challenges, including trustworthy client participation, inefficient model aggregation due to client with malicious or less accurate model. In this paper, we propose a trustworthy FL method incorporating Q-learning, trust, and reputation mechanisms, enhancing model accuracy and fairness. This method promotes client participation, mitigates malicious attacks' impact, and ensures fair model distribution. Inspired by reinforcement learning, the Q-learning algorithm optimizes client selection using the Bellman equation, enabling the server to balance exploration and exploitation for improved system performance. Furthermore, we explored the advantages of peer-to-peer FL settings. Extensive experimentation demonstrates our proposed trustworthy FL approach's effectiveness in achieving high learning accuracy while ensuring fairness across clients and maintaining efficient client selection. Our results reveal significant improvements in model performance, convergence speed, and generalization.",https://www.semanticscholar.org/paper/Empowering-Trustworthy-Client-Selection-in-Edge-Tariq-Lakas/582b74eebfc7abb4b1889fce53154fb4b74c7bfa
Efficient distributional reinforcement learning with Kullback-Leibler divergence regularization,"Renxing Li, Zhiwei Shang, Chunhuang Zheng, Huiyun Li, Qing Liang, Yunduan Cui","In this article, we address the issues of stability and data-efficiency in reinforcement learning (RL). A novel RL approach, Kullback-Leibler divergence-regularized distributional RL (KL-C51) is proposed to integrate the advantages of both stability in the distributional RL and data-efficiency in the Kullback-Leibler (KL) divergence-regularized RL in one framework. KL-C51 derived the Bellman equation and the TD errors regularized by KL divergence in a distributional perspective and explored the approximated strategies of properly mapping the corresponding Boltzmann softmax term into distributions. Evaluated not only by several benchmark tasks with different complexity from OpenAI Gym but also by six Atari 2600 games from the Arcade Learning Environment, the proposed method clearly illustrates the positive effect of the KL divergence regularization to the distributional RL including exclusive exploration behaviors and smooth value function update, and demonstrates an improvement in both learning stability and data-efficiency compared with other related baseline approaches.",https://www.semanticscholar.org/paper/Efficient-distributional-reinforcement-learning-Li-Shang/e65823e7e8c50f873abe8cd2a9433916b4a62f43
Reproducibility in Deep Reinforcement Learning with Maximum Entropy,"Tudor-Andrei Paleu, Carlos Pascal","The latest work in the field of deep reinforcement learning speaks highly about the advanced exploration techniques which avoid the greedy decisions of agents. Usually, reinforcement learning works by finding the optimal policy for a Markov Decision Process. In off-policy algorithms the agent learns a value function for this optimal policy, separate of the action choice, an example being the deep Q-learning algorithm. Algorithms based on a maximum entropy framework, like soft Q-learning, overcome the greedy behavior of the agent, effectively combining exploration and exploitation by adding an entropy term to the Bellman equation. This method, applied to the Lunar Lander environment, was compared to the classic deep Q-learning, using the same set of different random seeds and averaging multiple runs. An implicit exploration strategy proves to compensate for disturbances caused by intrinsic sources of non-determinism, such as random seeds. This paper highlights the sensitivity to intrinsic and extrinsic influences for deep reinforcement learning, with respect to exploration and repeatability.",https://www.semanticscholar.org/paper/Reproducibility-in-Deep-Reinforcement-Learning-with-Paleu-Pascal/73ef3cacf5263a5a9d7c06e86f4adef20fede29f
Dynamic Event-Triggered Robust Optimal Attitude Control of QUAV Using Reinforcement Learning,"Peng Jin, Qian Ma, Shengyuan Xu","In this article, a dynamic event-triggered robust optimal attitude tracking control problem for a quadrotor unmanned aerial vehicle in an uncertain environment is investigated. First, an augmented system consisting of tracking error signal and reference signal is developed to transform the tracking problem into a stabilization problem. Then, in order to handle the random disturbances in the design of the optimal controller, a new Hamilton–Jacobi–Bellman equation is proposed. Subsequently, the dynamic event-triggered mechanism is designed to reduce the communication pressure, and an event-based critic-only reinforcement learning algorithm is proposed to implement the optimal controller design. Remarkably, by combining concurrent learning technique and gradient descent algorithm, the adaptive weight update law is derived to tune the critic neural network, thus erasing the demand on the persistent excitation condition. After that, we demonstrate that the closed-loop system is semiglobally uniformly ultimately bounded in mean square and prove the Zeno-free behavior. Finally, the simulation results are given to show the effectiveness of our control strategy.",https://www.semanticscholar.org/paper/Dynamic-Event-Triggered-Robust-Optimal-Attitude-of-Jin-Ma/f75fb37057a7784e4c9e212a70950378c7a83945
Integral Reinforcement-Learning-Based Optimal Containment Control for Partially Unknown Nonlinear Multiagent Systems,"Qiuye Wu, Yongheng Wu, Yonghua Wang","This paper focuses on the optimal containment control problem for the nonlinear multiagent systems with partially unknown dynamics via an integral reinforcement learning algorithm. By employing integral reinforcement learning, the requirement of the drift dynamics is relaxed. The integral reinforcement learning method is proved to be equivalent to the model-based policy iteration, which guarantees the convergence of the proposed control algorithm. For each follower, the Hamilton–Jacobi–Bellman equation is solved by a single critic neural network with a modified updating law which guarantees the weight error dynamic to be asymptotically stable. Through using input–output data, the approximate optimal containment control protocol of each follower is obtained by applying the critic neural network. The closed-loop containment error system is guaranteed to be stable under the proposed optimal containment control scheme. Simulation results demonstrate the effectiveness of the presented control scheme.",https://www.semanticscholar.org/paper/Integral-Reinforcement-Learning-Based-Optimal-for-Wu-Wu/f70c9ea1bf760efac8f1e224b51477cb40afc65a
LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning,"Outongyi Lv, Bingxin Zhou, Yu Wang","Modern reinforcement learning (RL) can be categorized into online and offline variants. As a pivotal aspect of both online and offline RL, current research on the Bellman equation revolves primarily around optimization techniques and performance enhancement rather than exploring the inherent structural properties of the Bellman error, such as its distribution characteristics. This study investigates the distribution of the Bellman approximation error in both online and offline settings through iterative exploration of the Bellman equation. We observed that both in online RL and offline RL, the Bellman error conforms to a Logistic distribution. Building upon this discovery, this study employed the Logistics maximum likelihood function (LLoss) as an alternative to the commonly used MSE Loss, assuming that Bellman errors adhere to a normal distribution. We validated our hypotheses through extensive numerical experiments across diverse online and offline environments. In particular, we applied corrections to the loss function across various baseline algorithms and consistently observed that the loss function with Logistic corrections outperformed the MSE counterpart significantly. Additionally, we conducted Kolmogorov-Smirnov tests to confirm the reliability of the Logistic distribution. This study's theoretical and empirical insights provide valuable groundwork for future investigations and enhancements centered on the distribution of Bellman errors.",https://www.semanticscholar.org/paper/LLQL%3A-Logistic-Likelihood-Q-Learning-for-Learning-Lv-Zhou/3e72a33046892c13fc8deaa3628e01dc518348e5
Distributed Generative Multi-Adversarial Reinforcement Learning for Multi-Vehicle Pursuit,"Xinhang Li, Yiying Yang, +5 authors Prof. Lin Zhang","13 Multi-vehicle pursuit (MVP) is one of the most challenging problems for intelligent traffic management systems, due to 14 multi-source heterogeneous data and its mission nature. Many reinforcement learning (RL) algorithms have shown 15 promising abilities for the MVP problem. However, the sparse reward of pursuing tasks and the lack of effective traffic 16 awareness hinder the optimization of RL decisions and the improvement of pursuing efficiency. Therefore, this paper 17 proposes a Distributed Generative Multi-Adversarial Reinforcement Learning for Multi-Vehicle Pursuit (DGMARL-MVP) 18 in urban traffic scenes. In DGMARL-MVP, a generative multi-adversarial network (GMAN) is designed to improve 19 the Bellman equation by generating the potential dense reward, thereby properly guiding strategy optimization of 20 distributed multi-agent RL. Moreover, a graph neural network (GNN)-based intersecting cognition is proposed to 21 extract integrated features of traffic situations and agents’ relationships from multi-source heterogeneous data to 22 assist in improving the pursuing efficiency. Extensive experimental results show that the DGMARL-MVP can reduce 23 the pursuit time by 5.47 % compared with proximal policy optimization and improve pursuing average success rate up 24 to 85.67 % . Codes are open sourced in Github. 25",https://www.semanticscholar.org/paper/Distributed-Generative-Multi-Adversarial-Learning-Li-Yang/53b7fa06fd41328f8f149f7523f4d53410c1b46f
Safety-Aware Optimal Control of Nonlinear Systems Using Off-Policy Reinforcement Learning*,"Mingduo Lin, Bo Zhao, Hongbing Xia, Derong Liu","In this paper, we investigate the safety-aware optimal control (SAOC) problem, which attempts to minimize a predefined performance index function while ensuring the safety of nonlinear systems. First, the barrier function-based system transformation is utilized to design an optimal control policy which maintains the system states located in the safety region. To deal with the input constraints, a non-quadratic cost function is imposed to the control input. Then, the Hamilton-Jacobi-Bellman equation is established to provide the solution of the SAOC problem. Moreover, by utilizing the off-policy Bellman equation, a data-based off-policy reinforcement learning (OPRL) algorithm is developed to obtain the safety-aware optimal controller in a model-free manner. To implement this algorithm, a data collection process with the barrier transform is executed to generate the off-policy trajectory data, and an actor-critic neural network structure with the least-square updating law is employed in the off-policy learning phase. Finally, a simulation example is provided to verify the effectiveness of the developed control method.",https://www.semanticscholar.org/paper/Safety-Aware-Optimal-Control-of-Nonlinear-Systems-Lin-Zhao/c40e0c5503105a0e6f384e3badbc1ac9ba4ece28
OPTIMAL TRACKING CONTROL FOR ROBOT MANIPULATORS WITH ASYMMETRIC SATURATION TORQUES BASED ON REINFORCEMENT LEARNING,"N. D. Dien, Luy Tan Nguyen, L. Lãi"," This paper introduces an optimal tracking controller for robot manipulators with asymmetrically saturated torques and partially - unknown dynamics based on a reinforcement  learning method using a neural network. Firstly, the feedforward control inputs are designed based on the backstepping technique to convert the tracking control problem into the optimal tracking control problem. Secondly, a cost function of the system with asymmetrically saturated input is defined, and the constrained Hamilton-Jacobi- Bellman  equation is built, which is solved by the online reinforcement  learning algorithm using only a single neural network. Then, the asymmetric saturation optimal control rule is determined. Additionally, the concurrent learning technique is used to relax the demand for the persistence of excitation conditions. The built algorithm ensures that the closed-loop system is asymptotically stable, the approximation error is uniformly ultimately bounded (UUB), and the cost function converges to the near-optimal value. Finally, the effectiveness of the proposed algorithm is shown through comparative simulations. ",https://www.semanticscholar.org/paper/OPTIMAL-TRACKING-CONTROL-FOR-ROBOT-MANIPULATORS-ON-Dien-Nguyen/c1d7bebf3876642ec26145b57802e50ce1af9cea
Empowering Trustworthy Client Selection in Edge Federated Learning Leveraging Reinforcement Learning,"Asadullah Tariq, Abderrahmane Lakas, F. Sallabi, Tariq Qayyum, M. Serhani, Ezedin Baraka","Federated learning (FL) is a promising approach for training AI models across multiple clients in Edge Computing (EC), without sharing raw local data. By enabling local training and aggregating updates into a global model, FL maintains privacy while facilitating collaborative learning. Nevertheless, FL encounters several challenges, including trustworthy client participation, inefficient model aggregation due to client with malicious or less accurate model. In this paper, we propose a trustworthy FL method incorporating Q-learning, trust, and reputation mechanisms, enhancing model accuracy and fairness. This method promotes client participation, mitigates malicious attacks' impact, and ensures fair model distribution. Inspired by reinforcement learning, the Q-learning algorithm optimizes client selection using the Bellman equation, enabling the server to balance exploration and exploitation for improved system performance. Furthermore, we explored the advantages of peer-to-peer FL settings. Extensive experimentation demonstrates our proposed trustworthy FL approach's effectiveness in achieving high learning accuracy while ensuring fairness across clients and maintaining efficient client selection. Our results reveal significant improvements in model performance, convergence speed, and generalization.",https://www.semanticscholar.org/paper/Empowering-Trustworthy-Client-Selection-in-Edge-Tariq-Lakas/582b74eebfc7abb4b1889fce53154fb4b74c7bfa
Integral Reinforcement-Learning-Based Optimal Containment Control for Partially Unknown Nonlinear Multiagent Systems,"Qiuye Wu, Yongheng Wu, Yonghua Wang","This paper focuses on the optimal containment control problem for the nonlinear multiagent systems with partially unknown dynamics via an integral reinforcement learning algorithm. By employing integral reinforcement learning, the requirement of the drift dynamics is relaxed. The integral reinforcement learning method is proved to be equivalent to the model-based policy iteration, which guarantees the convergence of the proposed control algorithm. For each follower, the Hamilton–Jacobi–Bellman equation is solved by a single critic neural network with a modified updating law which guarantees the weight error dynamic to be asymptotically stable. Through using input–output data, the approximate optimal containment control protocol of each follower is obtained by applying the critic neural network. The closed-loop containment error system is guaranteed to be stable under the proposed optimal containment control scheme. Simulation results demonstrate the effectiveness of the presented control scheme.",https://www.semanticscholar.org/paper/Integral-Reinforcement-Learning-Based-Optimal-for-Wu-Wu/f70c9ea1bf760efac8f1e224b51477cb40afc65a
Efficient distributional reinforcement learning with Kullback-Leibler divergence regularization,"Renxing Li, Zhiwei Shang, Chunhuang Zheng, Huiyun Li, Qing Liang, Yunduan Cui","In this article, we address the issues of stability and data-efficiency in reinforcement learning (RL). A novel RL approach, Kullback-Leibler divergence-regularized distributional RL (KL-C51) is proposed to integrate the advantages of both stability in the distributional RL and data-efficiency in the Kullback-Leibler (KL) divergence-regularized RL in one framework. KL-C51 derived the Bellman equation and the TD errors regularized by KL divergence in a distributional perspective and explored the approximated strategies of properly mapping the corresponding Boltzmann softmax term into distributions. Evaluated not only by several benchmark tasks with different complexity from OpenAI Gym but also by six Atari 2600 games from the Arcade Learning Environment, the proposed method clearly illustrates the positive effect of the KL divergence regularization to the distributional RL including exclusive exploration behaviors and smooth value function update, and demonstrates an improvement in both learning stability and data-efficiency compared with other related baseline approaches.",https://www.semanticscholar.org/paper/Efficient-distributional-reinforcement-learning-Li-Shang/e65823e7e8c50f873abe8cd2a9433916b4a62f43
SAUTE RL: Almost Surely Safe Reinforcement Learning Using State Augmentation,"Aivar Sootla, A. Cowen-Rivers, +4 authors Haitham Bou-Ammar","Satisfying safety constraints almost surely (or with probability one) can be critical for the deployment of Reinforcement Learning (RL) in real-life applications. For example, plane landing and take-off should ideally occur with probability one. We address the problem by introducing Safety Augmented (Saute) Markov Decision Processes (MDPs), where the safety constraints are eliminated by augmenting them into the state-space and reshaping the objective. We show that Saute MDP satisfies the Bellman equation and moves us closer to solving Safe RL with constraints satisfied almost surely. We argue that Saute MDP allows viewing the Safe RL problem from a different perspective enabling new features. For instance, our approach has a plug-and-play nature, i.e., any RL algorithm can be""Sauteed"". Additionally, state augmentation allows for policy generalization across safety constraints. We finally show that Saute RL algorithms can outperform their state-of-the-art counterparts when constraint satisfaction is of high importance.",https://www.semanticscholar.org/paper/SAUTE-RL%3A-Almost-Surely-Safe-Reinforcement-Learning-Sootla-Cowen-Rivers/fda6c22149d269cb2730e919ed4254a5a5f84ae0
Reachability Analysis-based Safety-Critical Control using Online Fixed-Time Reinforcement Learning,"Nick-Marios T. Kokolakis, K. Vamvoudakis, W. Haddad","In this paper, we address a safety-critical control problem using reachability analysis and design a reinforcement learning-based mechanism for learning online and in fixed-time the solution to the safety-critical control problem. Safety is assured by determining a set of states for which there does not exist an admissible control law generating a system trajectory reaching a set of forbidden states at a user-prescribed time instant. Specifically, we cast our safety-critical problem as a Mayer optimal feedback control problem whose solution satisfies the Hamilton-Jacobi-Bellman (HJB) equation and characterizes the set of safe states. Since the HJB equation is generally difficult to solve, we develop an online critic-only reinforcement learning-based algorithm for simultaneously learning the solution to the HJB equation and the safe set in fixed time. In particular, we introduce a non-Lipschitz experience replay-based learning law utilizing recorded and current data for updating the critic weights to learn the value function and the safe set. The non-Lipschitz property of the dynamics gives rise to fixed-time convergence, whereas the experience replay-based approach eliminates the need of satisfying the persistence of excitation condition provided that the recorded data is sufficiently rich. Simulation results illustrate the efficacy of the proposed approach.",https://www.semanticscholar.org/paper/Reachability-Analysis-based-Safety-Critical-Control-Kokolakis-Vamvoudakis/364bf44c934d2a26b51f37865b4de7a173c881a2
Robust Near-optimal Control for Constrained Nonlinear System via Integral Reinforcement Learning,"Y. Qiu, Yan Li, Zhong Wang","This paper proposes a robust near-optimal control algorithm for uncertain nonlinear systems with state constraints and input saturation. By incorporating a barrier function and a non-quadratic term, the robust stabilization problem with constraints and uncertainties is converted into an unconstrained optimal control problem of the nominal system, which requires the solution of the Hamilton-Jacobi-Bellman (HJB) equation. The proposed integral reinforcement learning (IRL)-based method can obtain the approximate solution of the HJB equation without requiring any knowledge of system drift dynamics. An online gain-adjustable update law of the actor-critic architecture is developed to relax the persistence of excitation (PE) condition and ensure the closed-loop system stability throughout learning. The uniform ultimate boundedness of the closed-loop system is verified using Lyapunov’s direct method. Simulation results demonstrate the effectiveness and feasibility of the proposed method.",https://www.semanticscholar.org/paper/Robust-Near-optimal-Control-for-Constrained-System-Qiu-Li/790ee8f4bcdf53295c0d3981adbb17dbad7fcae6
Reinforcement Learning With Non-Cumulative Objective,"W. Cui, Wei Yu","In reinforcement learning, the objective is almost always defined as a cumulative function over the rewards along the process. However, there are many optimal control and reinforcement learning problems in various application fields, especially in communications and networking, where the objectives are not naturally expressed as summations of the rewards. In this paper, we recognize the prevalence of non-cumulative objectives in various problems, and propose a modification to existing algorithms for optimizing such objectives. Specifically, we dive into the fundamental building block for many optimal control and reinforcement learning algorithms: the Bellman optimality equation. To optimize a non-cumulative objective, we replace the original summation operation in the Bellman update rule with a generalized operation corresponding to the objective. Furthermore, we provide sufficient conditions on the form of the generalized operation as well as assumptions on the Markov decision process under which the globally optimal convergence of the generalized Bellman updates can be guaranteed. We demonstrate the idea experimentally with the bottleneck objective, i.e., the objectives determined by the minimum reward along the process, on classical optimal control and reinforcement learning tasks, as well as on two network routing problems on maximizing the flow rates.",https://www.semanticscholar.org/paper/Reinforcement-Learning-With-Non-Cumulative-Cui-Yu/ed0708e28d9a8e9b01e9c3f4cf6b22432ae36e1e
OPTIMAL TRACKING CONTROL FOR ROBOT MANIPULATORS WITH INPUT CONSTRAINT BASED REINFORCEMENT LEARNING,"N. D. Dien, Luy Tan Nguyen, L. Lãi, Tran Thanh Hai","This paper introduces an optimal tracking controller for robot manipulators with saturation torques. The robot model is presented as a strict-feedback nonlinear system. Firstly, the position tracking control problem is transformed into the optimal tracking control problem. Subsequently, the saturated optimal control law is designed. The optimal control law is determined through the solution of the Hamilton-Jacobi-Bellman (HJB) equation. We use a reinforcement learning algorithm with only one neural network (NN) to approximate the solution of the equation HJB. The technique of experience replay is used to relax a persistent citation condition. By Lyapunov analysis, the tracking and the approximation errors are uniformly ultimately bounded (UUB). Finally, the simulation on a robot manipulator with saturation torques is performed to verify the efficiency of the proposed controller.",https://www.semanticscholar.org/paper/OPTIMAL-TRACKING-CONTROL-FOR-ROBOT-MANIPULATORS-Dien-Nguyen/7ba77c1cb97bcc7f8c15190c49fc077a2e657357
Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning,"Peizhong Ju, A. Ghosh, N. Shroff","Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence region. We show that such an approach achieves sub-linear regret in terms of the number of episodes. Additionally, we provide a probably approximately correct (PAC) guarantee based on the obtained regret bound. We also propose an offline RL algorithm and bound the optimality gap with respect to the optimal fair solution. To mitigate computational complexity, we introduce a policy-gradient type method for the fair objective. Simulation experiments also demonstrate the efficacy of our approach.",https://www.semanticscholar.org/paper/Achieving-Fairness-in-Multi-Agent-Markov-Decision-Ju-Ghosh/058c472c6d984e38ff74170c79b7e4556f24e3d4
Model-Free Robust Average-Reward Reinforcement Learning,"Yue Wang, Alvaro Velasquez, George K. Atia, Ashley Prater-Bennette, Shaofeng Zou","Robust Markov decision processes (MDPs) address the challenge of model uncertainty by optimizing the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on the robust average-reward MDPs under the model-free setting. We first theoretically characterize the structure of solutions to the robust average-reward Bellman equation, which is essential for our later convergence analysis. We then design two model-free algorithms, robust relative value iteration (RVI) TD and robust RVI Q-learning, and theoretically prove their convergence to the optimal solution. We provide several widely used uncertainty sets as examples, including those defined by the contamination model, total variation, Chi-squared divergence, Kullback-Leibler (KL) divergence and Wasserstein distance.",https://www.semanticscholar.org/paper/Model-Free-Robust-Average-Reward-Reinforcement-Wang-Velasquez/28c2c876068437a056802083a73157a1b5c1f9e2
